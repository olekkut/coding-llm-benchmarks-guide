# coding-llm-benchmarks-guide
This repository provides an extensive, in-depth comparison and benchmarking of state-of-the-art local coding Large Language Models (LLMs), evaluating their performance, accuracy, inference speed, memory requirements, supported programming languages, licensing, and practical suitability for various use cases.

# Comparison of Local Coding Models

## Model Overview and Key Differences
- **DeepSeek Coder v2 (236B & 16B)** – Open Mixture-of-Experts (MoE) code models that achieve **GPT-4 level coding performance** ([GitHub - deepseek-ai/DeepSeek-Coder-V2: DeepSeek-Coder-V2: Breaking the Barrier of Closed-Source Models in Code Intelligence](https://github.com/deepseek-ai/DeepSeek-Coder-V2#:~:text=We%20present%20DeepSeek,V2%20expands%20its%20support%20for)). The 236B version uses 21B “active” parameters (via MoE) and supports a **128K context** ([GitHub - deepseek-ai/DeepSeek-Coder-V2: DeepSeek-Coder-V2: Breaking the Barrier of Closed-Source Models in Code Intelligence](https://github.com/deepseek-ai/DeepSeek-Coder-V2#:~:text=the%20coding%20and%20mathematical%20reasoning,length%20from%2016K%20to%20128K)) ([GitHub - deepseek-ai/DeepSeek-Coder-V2: DeepSeek-Coder-V2: Breaking the Barrier of Closed-Source Models in Code Intelligence](https://github.com/deepseek-ai/DeepSeek-Coder-V2#:~:text=We%20release%20the%20DeepSeek,instruct%20models%2C%20to%20the%20public)), dramatically expanding from 86 to **338 programming languages** supported ([GitHub - deepseek-ai/DeepSeek-Coder-V2: DeepSeek-Coder-V2: Breaking the Barrier of Closed-Source Models in Code Intelligence](https://github.com/deepseek-ai/DeepSeek-Coder-V2#:~:text=the%20coding%20and%20mathematical%20reasoning,length%20from%2016K%20to%20128K)). The 16B “Lite” model uses 2.4B active params for faster, lighter deployment ([GitHub - deepseek-ai/DeepSeek-Coder-V2: DeepSeek-Coder-V2: Breaking the Barrier of Closed-Source Models in Code Intelligence](https://github.com/deepseek-ai/DeepSeek-Coder-V2#:~:text=2)). Both are fine-tuned for code instructions and available under an open model license (allowing commercial use) ([deepseek-ai/deepseek-coder-33b-instruct · Hugging Face](https://huggingface.co/deepseek-ai/deepseek-coder-33b-instruct#:~:text=4)).

- **DeepSeek Coder (33B, 6.7B, 1.3B)** – Earlier DeepSeek models (33B instruct and smaller variants) trained on **2 trillion tokens** (87% code, 13% English/Chinese) ([deepseek-ai/deepseek-coder-33b-instruct · Hugging Face](https://huggingface.co/deepseek-ai/deepseek-coder-33b-instruct#:~:text=Deepseek%20Coder%20is%20composed%20of,programming%20languages%20and%20various%20benchmarks)). They offer **16K context windows** and state-of-the-art accuracy among open models of their time ([deepseek-ai/deepseek-coder-33b-instruct · Hugging Face](https://huggingface.co/deepseek-ai/deepseek-coder-33b-instruct#:~:text=capabilities%2C%20Deepseek%20Coder%20achieves%20state,programming%20languages%20and%20various%20benchmarks)). The 33B was a top performer on benchmarks like HumanEval and MBPP ([blog/codegemma.md at main · huggingface/blog · GitHub](https://github.com/huggingface/blog/blob/main/codegemma.md#:~:text=CodeGemma,leaderboard%20or%20some%20metrics%20below)), while the 6.7B (≈7B) model outperformed other 7B models on Python code tasks ([blog/codegemma.md at main · huggingface/blog · GitHub](https://github.com/huggingface/blog/blob/main/codegemma.md#:~:text=CodeGemma,leaderboard%20or%20some%20metrics%20below)). These models are flexible (multiple sizes) and permit commercial use under DeepSeek’s model license ([deepseek-ai/deepseek-coder-33b-instruct · Hugging Face](https://huggingface.co/deepseek-ai/deepseek-coder-33b-instruct#:~:text=4)).

- **Qwen 2.5 Coder (32B, 14B, 7B)** – Alibaba’s Qwen2.5-Coder family of code-specialized models (Apache 2.0 licensed ([qwen2.5-coder-7b-instruct Model by Qwen - NVIDIA NIM APIs](https://build.nvidia.com/qwen/qwen2_5-coder-7b-instruct/modelcard#:~:text=qwen2.5,0%20License))). The flagship **32B-Instruct** model is currently **SOTA among open models**, **matching GPT-4 (GPT-4o)** on code generation benchmarks ([Qwen2.5-Coder Series: Powerful, Diverse, Practical. | Qwen](https://qwenlm.github.io/blog/qwen2.5-coder-family/#:~:text=%2A%20Powerful%3A%20Qwen2.5,Coder%20in%20two%20scenarios)) ([Qwen2.5-Coder Series: Powerful, Diverse, Practical. | Qwen](https://qwenlm.github.io/blog/qwen2.5-coder-family/#:~:text=Image)). It excels at code repair (e.g. Aider score 73.7, similar to GPT-4o) ([Qwen2.5-Coder Series: Powerful, Diverse, Practical. | Qwen](https://qwenlm.github.io/blog/qwen2.5-coder-family/#:~:text=%2A%20Code%20Generation%3A%20Qwen2.5,4o)) and handles **40+ programming languages** (strong even in Haskell, Racket, etc.) ([Qwen2.5-Coder Series: Powerful, Diverse, Practical. | Qwen](https://qwenlm.github.io/blog/qwen2.5-coder-family/#:~:text=,training%20phase)). The 7B and 14B models offer competitive mid-size performance (with the 7B often **outperforming the 14B** on some benchmarks ([Qwen 2.5 Coder 14b is worse than 7b on several benchmarks in the technical report - weird! : r/LocalLLaMA](https://www.reddit.com/r/LocalLLaMA/comments/1gpriif/qwen_25_coder_14b_is_worse_than_7b_on_several/#:~:text=https%3A%2F%2Farxiv))) and support up to **32K context** length ([qwen2.5-coder-7b-instruct Model by Qwen - NVIDIA NIM APIs](https://build.nvidia.com/qwen/qwen2_5-coder-7b-instruct/modelcard#:~:text=qwen2.5,0%20License)). All Qwen coder models support fine-tuning and are **free for commercial use** (Apache 2.0).

- **CodeLlama (70B, 34B, 13B, 7B)** – Meta’s family of Llama2-based code models, released under a permissive community license for research and commercial use ([[2308.12950] Code Llama: Open Foundation Models for Code](https://arxiv.org/abs/2308.12950#:~:text=art%20performance%20among%20open%20models,both%20research%20and%20commercial%20use)). They come in base, Python-tuned, and instruct variants. All support **16K context windows** (with effective performance even on 100K inputs) ([[2308.12950] Code Llama: Open Foundation Models for Code](https://arxiv.org/abs/2308.12950#:~:text=to%20cover%20a%20wide%20range,and%20all%20our%20models%20outperform)) and **infilling** for code completion. CodeLlama models set a new standard upon release, with **CodeLlama-70B scoring ~67-68% on HumanEval** ([Meta releases Code Llama2-70B, claims 67+ Humaneval - Reddit](https://www.reddit.com/r/LocalLLaMA/comments/1ae13f7/meta_releases_code_llama270b_claims_67_humaneval/#:~:text=Reddit%20www.reddit.com%20%20CodeLlama,70B%20is%20the)) – one of the best at the time – and even the 7B model outperforming a 70B general Llama2 on code tasks ([[2308.12950] Code Llama: Open Foundation Models for Code](https://arxiv.org/abs/2308.12950#:~:text=100k%20tokens,both%20research%20and%20commercial%20use)). Strengths include robust instruction following and broad applicability, though their accuracy is now surpassed by newer specialized coders. The **license allows commercial use**, with some usage restrictions similar to Llama2.

- **CodeGemma (7B & 2B)** – Google’s **lightweight open code models** built on Gemma. The 7B model (pretrained and an **instruction-tuned** variant) shows strong natural language understanding, math reasoning, and code capabilities on par with other open 7Bs ([[2406.11409] CodeGemma: Open Code Models Based on Gemma](https://arxiv.org/abs/2406.11409#:~:text=code%20models%20built%20on%20top,sensitive%20settings)). In fact, **CodeGemma-7B outperforms most 7B peers (except DeepSeek-7B) on HumanEval** ([blog/codegemma.md at main · huggingface/blog · GitHub](https://github.com/huggingface/blog/blob/main/codegemma.md#:~:text=CodeGemma,leaderboard%20or%20some%20metrics%20below)) and MultiPL-E, and its instruct version further boosts popular language performance ([blog/codegemma.md at main · huggingface/blog · GitHub](https://github.com/huggingface/blog/blob/main/codegemma.md#:~:text=CodeGemma,leaderboard%20or%20some%20metrics%20below)). The 2B model is specialized for **fast code completion** (100% code infilling objective) ([blog/codegemma.md at main · huggingface/blog · GitHub](https://github.com/huggingface/blog/blob/main/codegemma.md#:~:text=CodeGemma%20is%20a%20family%20of,for%20code%20completion%20and%20generation)), making it ideal for low-latency IDE integration. All CodeGemma models use an **8K context** window ([blog/codegemma.md at main · huggingface/blog · GitHub](https://github.com/huggingface/blog/blob/main/codegemma.md#:~:text=CodeGemma%202B%20was%20trained%20exclusively,context%20size%20as%20their%20predecessors)). They are available after accepting Google’s Gemma terms (permit **responsible commercial use for all organizations** ([Gemma: Introducing new state-of-the-art open models](https://blog.google/technology/developers/gemma-open-models/#:~:text=Gemma%3A%20Introducing%20new%20state,all%20organizations%2C%20regardless%20of%20size))). 

- **Command R+ (111B)** – Cohere’s large language model (predecessor to Command A). Command R+ (c.104–111B parameters) is a dense general-purpose model fine-tuned for **chat and long-context tasks** ([Cohere's Command R+ Model (Details and Application)](https://docs.cohere.com/v2/docs/command-r-plus#:~:text=Command%20R%2B%20is%20Cohere%27s%20model,step%20tool%20use)), including coding. It supports very long inputs (R+ had up to 100K tokens; the newer Command A extends to **256K context** ([Command A — Cohere](https://docs.cohere.com/v2/docs/command-a#:~:text=enterprise%20tasks%20including%20tool%20use%2C,2024))). This model is known for strong performance in complex reasoning and tool use, and its code ability improved significantly in the August 2024 R+ release (with “big gains in math and code”) ([C4AI Command A 111B : r/LocalLLaMA](https://www.reddit.com/r/LocalLLaMA/comments/1jabin6/c4ai_command_a_111b/#:~:text=%E2%80%A2)). However, it is released under a **non-commercial research license** ([C4AI Command A 111B : r/LocalLLaMA](https://www.reddit.com/r/LocalLLaMA/comments/1jabin6/c4ai_command_a_111b/#:~:text=%E2%80%A2)). Running it requires substantial compute (Cohere’s 111B can run on **two A100/H100 GPUs** with optimized inference ([Command A — Cohere](https://docs.cohere.com/v2/docs/command-a#:~:text=enterprise%20tasks%20including%20tool%20use%2C,2024))). Fine-tuning is possible (open weights), but any production use would require a special license from Cohere.

- **Codestral (22B)** – Mistral AI’s 22B parameter **code-focused model**. It’s trained on **80+ programming languages** ([Codestral | Mistral AI](https://mistral.ai/news/codestral#:~:text=A%20model%20fluent%20in%2080%2B,programming%20languages)) and designed for both instruction and completion use. Despite its moderate size, Codestral achieves **top-tier code generation performance**. It outperforms larger models like CodeLlama-70B and DeepSeek-33B on several benchmarks, aided by an extensive **32K context window** ([Codestral | Mistral AI](https://mistral.ai/news/codestral#:~:text=Performance,previous%20models%20used%20for%20coding)) ([How Codestral 22B is Leading the Charge in AI Code Generation](https://www.analyticsvidhya.com/blog/2024/06/codestral-22b/#:~:text=One%20of%20the%20standout%20features,performance%20on%20the%20RepoBench%20benchmark)) for long-range code completion. For example, it scores ~**81% on HumanEval (Python)** and handles multi-language HumanEval tasks with an average 61.5% success, leading other open models in that category. The model is **open-weight** but under a **“Non-Production” license ([Codestral | Mistral AI](https://mistral.ai/news/codestral#:~:text=Codestral%20is%20a%2022B%20open,can%20be%20downloaded%20on%20HuggingFace))** – free for research/testing, with commercial licenses available on request ([Codestral | Mistral AI](https://mistral.ai/news/codestral#:~:text=Codestral%20is%20a%2022B%20open,can%20be%20downloaded%20on%20HuggingFace)). This makes it a strong candidate for experimentation and internal use when 32K context or multi-language support is needed.

- **Mistral Large (123B)** – Mistral AI’s dense **flagship 123B model** (sometimes referred to as Mistral 2). It excels in **code generation, math, and reasoning** and comes with a massive **128K context window** ([mistral-large:123b/license](https://ollama.com/library/mistral-large:123b/blobs/9bac90050d38#:~:text=Mistral%20Large%202%20is%20Mistral%27s,support%20for%20dozens%20of%20languages)). Mistral 123B is positioned as a general-purpose model with performance comparable to other large-scale LLMs (it’s been compared roughly to Llama2/3 70B level on many tasks ([Mistral 123B vs LLAMA-3 405B, Thoughts? : r/LocalLLaMA - Reddit](https://www.reddit.com/r/LocalLLaMA/comments/1f2x9a5/mistral_123b_vs_llama3_405b_thoughts/#:~:text=Mistral%20123B%20vs%20LLAMA,compared%20with%2070B%20instead%20imo)), though with improved creativity and multilingual support). It is **open weight** but released under the Mistral AI Research License (non-commercial use by default) ([Codestral | Mistral AI](https://mistral.ai/news/codestral#:~:text=Codestral%20is%20a%2022B%20open,can%20be%20downloaded%20on%20HuggingFace)). Fine-tuning (e.g. via QLoRA) has been demonstrated on single high-end machines for this model, but the full model requires significant memory (≈70+ GB just for 8-bit weights) and multi-GPU inference. Mistral Large is best viewed as an open research platform approaching the upper end of LLM capabilities in coding and reasoning tasks.

## Performance and Accuracy
 ([Codestral | Mistral AI](https://mistral.ai/news/codestral)) *Benchmark comparison of select models on code tasks (higher is better). Codestral-22B delivers performance close to much larger models like CodeLlama-70B and DeepSeek Coder-33B on Python (HumanEval, MBPP) and even exceeds them on some long-context and multi-language code benchmarks ([Codestral | Mistral AI](https://mistral.ai/news/codestral#:~:text=Performance,previous%20models%20used%20for%20coding)) ([Codestral | Mistral AI](https://mistral.ai/news/codestral#:~:text=Download%20and%20test%20Codestral)).* 

When it comes to **code generation accuracy**, model size and specialization matter. At the highest end, **DeepSeek Coder v2-236B** attains near state-of-the-art results – **about 90% on HumanEval** (Python function generation) – essentially matching GPT-4’s level on that benchmark ([GitHub - deepseek-ai/DeepSeek-Coder-V2: DeepSeek-Coder-V2: Breaking the Barrier of Closed-Source Models in Code Intelligence](https://github.com/deepseek-ai/DeepSeek-Coder-V2#:~:text=CodeStral%2022B%2022B%2078,1)). This currently makes it the most accurate open model for coding (especially in Python and algorithmic challenges), also excelling in math reasoning and code-intensive challenges ([GitHub - deepseek-ai/DeepSeek-Coder-V2: DeepSeek-Coder-V2: Breaking the Barrier of Closed-Source Models in Code Intelligence](https://github.com/deepseek-ai/DeepSeek-Coder-V2#:~:text=We%20present%20DeepSeek,V2%20expands%20its%20support%20for)) ([GitHub - deepseek-ai/DeepSeek-Coder-V2: DeepSeek-Coder-V2: Breaking the Barrier of Closed-Source Models in Code Intelligence](https://github.com/deepseek-ai/DeepSeek-Coder-V2#:~:text=CodeStral%2022B%2022B%2017,7)). The trade-off is diminishing returns in size: other models with far fewer parameters are not far behind. For example, **Qwen2.5-Coder-32B** reportedly achieves **OpenAI GPT-4 Open (GPT-4o) parity** on many code benchmarks ([Qwen2.5-Coder Series: Powerful, Diverse, Practical. | Qwen](https://qwenlm.github.io/blog/qwen2.5-coder-family/#:~:text=%2A%20Powerful%3A%20Qwen2.5,Coder%20in%20two%20scenarios)) – it ranks at the top of EvalPlus, LiveCodeBench, BigCodeBench, etc., as of late 2024. While exact pass@1 numbers for Qwen-32B vary by task, it’s safe to say it scores in the high 70s to 80s on HumanEval (and notably **73.7 on code repair (Aider)**, versus GPT-4o ~75) ([Qwen2.5-Coder Series: Powerful, Diverse, Practical. | Qwen](https://qwenlm.github.io/blog/qwen2.5-coder-family/#:~:text=%2A%20Code%20Generation%3A%20Qwen2.5,4o)). In practice, Qwen-32B and DeepSeek-v2 (16B/236B) are within striking distance of each other on many coding tasks; DeepSeek’s 236B may edge out in pure code generation, whereas Qwen-32B is a **strong all-around coder** with open licensing.

Among mid-sized models (~30–70B), we see solid performance as well. **CodeLlama-70B** was a leader upon release, with about **67–68% pass@1 on HumanEval** ([Meta releases Code Llama2-70B, claims 67+ Humaneval - Reddit](https://www.reddit.com/r/LocalLLaMA/comments/1ae13f7/meta_releases_code_llama270b_claims_67_humaneval/#:~:text=Reddit%20www.reddit.com%20%20CodeLlama,70B%20is%20the)) and similar on MBPP (Python snippets) ([[2308.12950] Code Llama: Open Foundation Models for Code](https://arxiv.org/abs/2308.12950#:~:text=100k%20tokens,both%20research%20and%20commercial%20use)). It remains very capable, but newer 30B-class coders have surpassed it on code benchmarks. For instance, Codestral-22B reaches **81.1% on HumanEval (Python)** – significantly above CodeLlama-70B – thanks to its code-centric training. DeepSeek’s original **33B** model is in the same league, scoring ~77–79% on HumanEval and over 80% on MBPP. In fact, as of mid-2024, DeepSeek-33B Instruct was **state-of-the-art among open models** on many code benchmarks ([deepseek-ai/deepseek-coder-33b-instruct · Hugging Face](https://huggingface.co/deepseek-ai/deepseek-coder-33b-instruct#:~:text=capabilities%2C%20Deepseek%20Coder%20achieves%20state,programming%20languages%20and%20various%20benchmarks)), and it still holds up well against newer entrants. **Mistral-123B** (if used in instruct form) has been noted to perform roughly on par with these 30–70B models on coding tasks ([Mistral 123B vs LLAMA-3 405B, Thoughts? : r/LocalLLaMA - Reddit](https://www.reddit.com/r/LocalLLaMA/comments/1f2x9a5/mistral_123b_vs_llama3_405b_thoughts/#:~:text=Mistral%20123B%20vs%20LLAMA,compared%20with%2070B%20instead%20imo)) – impressive given it’s a general model – though exact benchmark figures aren’t formally published. Cohere’s **Command R+ (111B)** also demonstrated strong coding ability (one user benchmark noted it as roughly “mistral large level” in code/math) ([C4AI Command A 111B : r/LocalLLaMA](https://www.reddit.com/r/LocalLLaMA/comments/1jabin6/c4ai_command_a_111b/#:~:text=%E2%80%A2)); an updated Command-A model further improved on code, but these are limited to non-commercial uses.

For smaller models (<=15B), specialization and data quality often trump sheer size. **DeepSeek Coder 6.7B** and **CodeGemma-7B** both outperform the standard CodeLlama-7B by a wide margin. DeepSeek’s 6.7B was topping the 7B charts with ~45-46% on HumanEval (Python) ([blog/codegemma.md at main · huggingface/blog · GitHub](https://github.com/huggingface/blog/blob/main/codegemma.md#:~:text=CodeGemma,leaderboard%20or%20some%20metrics%20below)), where CodeLlama-7B was around 30%. Google’s CodeGemma-7B isn’t far behind DeepSeek – ~40% on Python HumanEval ([blog/codegemma.md at main · huggingface/blog · GitHub](https://github.com/huggingface/blog/blob/main/codegemma.md#:~:text=CodeGemma,leaderboard%20or%20some%20metrics%20below)) – and actually surpasses other 7B models like StarCoder 7B and CodeLlama in multiple languages. These small models also have surprisingly good general understanding (thanks to some natural language pretraining mix in CodeGemma’s case ([blog/codegemma.md at main · huggingface/blog · GitHub](https://github.com/huggingface/blog/blob/main/codegemma.md#:~:text=CodeGemma%20is%20a%20family%20of,for%20code%20completion%20and%20generation)) and DeepSeek’s inclusion of English/Chinese). **Qwen2.5-Coder-7B** likewise delivers impressive results for its size (comparable to CodeGemma’s ballpark, though slightly behind DeepSeek-7B). Notably, Qwen’s **14B** model had a quirk where it underperformed the 7B on some benchmarks ([Qwen 2.5 Coder 14b is worse than 7b on several benchmarks in the technical report - weird! : r/LocalLLaMA](https://www.reddit.com/r/LocalLLaMA/comments/1gpriif/qwen_25_coder_14b_is_worse_than_7b_on_several/#:~:text=https%3A%2F%2Farxiv)), but that seems to be a specific dip (possibly resolved in instruct tuning). In broad terms, a well-trained 7B–13B code model today can achieve ~40–50% on HumanEval, sufficient for simple functions and boilerplate generation, but will struggle with more complex problems where the larger models (70B, 100B+) push above 70–80%. 

In summary, **DeepSeek-Coder-v2 236B** currently leads for maximal accuracy (approaching closed-source quality), while **Qwen-32B** and **Codestral-22B** offer near-SOTA code performance at a fraction of that scale. Models like **DeepSeek-33B, Mistral-123B, Command R+/A, and CodeLlama-70B** populate the next tier – high-performing and capable on most tasks, usually within ~10-20 points of the leaders on pass@1 metrics. Meanwhile, **7B–16B models** (DeepSeek-7B, CodeGemma-7B, Qwen-7B, CodeLlama-13B, etc.) provide solid accuracy for simpler coding needs, albeit with a noticeable gap in success rate on harder prompts. The **optimal choice** depends on whether those last few percentage points are mission-critical, or if a smaller model can be fine-tuned to meet the target accuracy.

## Inference Speed and Memory Requirements
**Running these models locally** requires very different levels of hardware. Larger dense models (70B, 123B, 111B, 236B total parameters) need multiple high-memory GPUs or aggressive quantization. For example, a 70B model like CodeLlama typically needs ~140 GB in FP16 (which can be cut down to ~35 GB with 4-bit quantization, fitting on a 48 GB GPU). Mistral-123B at FP16 would require on the order of 250 GB of memory, making 8-bit (≈125 GB) or 4-bit (~63 GB) quantization almost mandatory for single-machine use – effectively requiring a multi-GPU server. Cohere’s **Command R+ 111B** has been optimized so that the new 111B version **runs with 2×80GB GPUs** for a massive 256K context ([Command A — Cohere](https://docs.cohere.com/v2/docs/command-a#:~:text=enterprise%20tasks%20including%20tool%20use%2C,2024)). In general, any 100B+ model is not real-time on a single consumer GPU – you’d be looking at single-digit tokens per second generation even with quantization, due to sheer model size.

The **mid-size models (20B–40B)** are much more tractable. A 30B model in 16-bit floats is ~60 GB; in 4-bit integer form, around ~16 GB. This means models like **Qwen-14B, Qwen-32B, Codestral-22B, DeepSeek-33B, CodeLlama-34B** can be run on a single modern GPU (e.g. a 24 GB RTX 4090 or 48 GB A6000) with quantization. In fact, Qwen-32B in 4-bit weighs about 20 GB ([qwen2.5-coder](https://ollama.com/library/qwen2.5-coder#:~:text=7b)) ([qwen2.5-coder](https://ollama.com/library/qwen2.5-coder#:~:text=There%20are%20significant%20improvements%20in,4o)), which was demonstrated running on Apple M1 Pro hardware smoothly ([Rudrank Riyam on X: "It is CRAZY how good the Qwen 2.5 Coder ...](https://x.com/rudrankriyam/status/1856050845403754575#:~:text=Rudrank%20Riyam%20on%20X%3A%20,device%20would%20be)). These models often achieve **5–10 tokens/sec** on high-end consumer GPUs when quantized. Codestral-22B, for instance, strikes a great balance: its **22B size is small enough** to yield higher throughput and lower latency than 70B models, yet it still **outperforms some 70B models** in quality ([Codestral | Mistral AI](https://mistral.ai/news/codestral#:~:text=Performance,previous%20models%20used%20for%20coding)). This **performance-per-compute sweet spot** ([Codestral | Mistral AI](https://mistral.ai/news/codestral#:~:text=Performance,previous%20models%20used%20for%20coding)) is a big draw of the ~20B range. Similarly, DeepSeek Coder v2’s **16B (2.4B active)** MoE model is designed to be efficient – at inference it only computes 2.4B parameters’ worth of activations per token ([GitHub - deepseek-ai/DeepSeek-Coder-V2: DeepSeek-Coder-V2: Breaking the Barrier of Closed-Source Models in Code Intelligence](https://github.com/deepseek-ai/DeepSeek-Coder-V2#:~:text=2)), so its speed is comparable to a 2.4B dense model (very fast) even though it delivers 16B-level accuracy. The catch is MoE’s memory overhead: one must still store all experts (236B total in the large model). DeepSeek v2’s 236B model is effectively **21B active per token** ([GitHub - deepseek-ai/DeepSeek-Coder-V2: DeepSeek-Coder-V2: Breaking the Barrier of Closed-Source Models in Code Intelligence](https://github.com/deepseek-ai/DeepSeek-Coder-V2#:~:text=2)) – implying that if the MoE is implemented efficiently, its **compute cost is akin to ~21B param model per token**, which is much lower than a dense 236B. This architecture can greatly accelerate inference *if* you have the hardware to hold the full model in memory (likely requiring a multi-GPU setup or TPU). In summary, **MoE models trade runtime compute for memory** – faster generation, but you can’t swap out the memory requirement of housing hundreds of billions of weights.

For **smaller models (<=13B)**, running locally is easy. These can often run on CPU or a single modest GPU. A 7B model in 4-bit can fit in ~4 GB of VRAM, meaning even a laptop GPU or Raspberry Pi with 8 GB RAM (using optimized libraries) can handle it. **CodeGemma-2B** is extremely light – on the order of 4 GB in 16-bit – so it can even run in memory-constrained environments or inferences on CPU with tolerable speed. You might see **15-30 tokens/sec** from a 7B on a decent GPU, and much higher for a 2B model. The **trade-off** is that smaller models may need to generate more and rely on iterative prompting (or require multiple attempts) to get correct solutions, whereas larger models often get it right in one shot due to higher accuracy.

It’s also worth noting **context length impact on memory and speed**. Models with extended context (16K, 32K, 128K) will consume more memory proportional to the context length during attention computation. For instance, CodeLlama’s 16K support means at full 16K usage it will run slower and need more memory than at 4K, due to the quadratic cost of self-attention. **Command R+/A’s 256K context** is a standout feature ([Command A — Cohere](https://docs.cohere.com/v2/docs/command-a#:~:text=enterprise%20tasks%20including%20tool%20use%2C,2024)), but exercising it fully would dramatically slow per-token speed (Cohere likely uses an RMT or segmented attention approach to mitigate this). **Mistral-123B and DeepSeek-v2** both boast 128K context windows ([GitHub - deepseek-ai/DeepSeek-Coder-V2: DeepSeek-Coder-V2: Breaking the Barrier of Closed-Source Models in Code Intelligence](https://github.com/deepseek-ai/DeepSeek-Coder-V2#:~:text=the%20coding%20and%20mathematical%20reasoning,length%20from%2016K%20to%20128K)) ([mistral-large:123b/license](https://ollama.com/library/mistral-large:123b/blobs/9bac90050d38#:~:text=Mistral%20Large%202%20is%20Mistral%27s,support%20for%20dozens%20of%20languages)) – extremely useful for analyzing large codebases or logs. However, not every use will require such length; for shorter contexts, these models can be run faster (or you might load a 128K model with a smaller rotary embedding without using full length, saving memory). **Codestral-22B’s 32K context** ([Codestral | Mistral AI](https://mistral.ai/news/codestral#:~:text=Figure%201%3A%20With%20its%20larger,range%20eval%20for%20code%20generation)) is a big plus for repository-level completions – and it explicitly shines on the RepoBench eval (long-range code completion) compared to models limited to 4K/8K ([Codestral | Mistral AI](https://mistral.ai/news/codestral#:~:text=Image%3A%20Detailed%20benchmarks)). Just be mindful that pushing any model to its max context will tax GPU memory (for example, 32K context uses 4× the memory of 8K for attention states).

In practical terms, **optimal deployments** might use quantization and context truncation to fit models on available hardware. If you have a single 16 GB GPU, models up to ~13B (8-bit) or ~30B (4-bit) are feasible. With a 24 GB GPU, you can try up to ~34B (8-bit) or ~70B (4-bit). Multi-GPU (2×24 GB) can handle 70B in higher precision or even 100B+ with 4-bit. And if you’re lucky enough to have **TPUs or multi-node A100s**, you can run the 100B+ dense or MoE models at decent speeds. Always consider the model’s **active compute** (MoE vs dense) and context requirement relative to your hardware for the best experience.

## Fine-Tuning and Customization
All these models are available as open weights, meaning you can fine-tune or adjust them to your specific coding domain – though some come pre-fine-tuned (instruct versions) which may suffice out-of-the-box. Most provide both a base model (raw pretraining on code) and an **instruction-tuned** variant (fine-tuned for helpful answers or chat). For example, DeepSeek offers base and instruct for each size, Qwen has instruct versions (e.g. Qwen-7B-Coder-**Instruct**), CodeLlama has base, Python, and instruct, and CodeGemma released a 7B **Instruct** on top of its base ([blog/codegemma.md at main · huggingface/blog · GitHub](https://github.com/huggingface/blog/blob/main/codegemma.md#:~:text=CodeGemma%202B%20was%20trained%20exclusively,context%20size%20as%20their%20predecessors)). If your use case is interactive coding assistance or following natural language prompts, you’ll want the instruct/chat model; if it’s pure code completion in an editor, the base model might perform better (since it won’t insert extra conversational text).

**Fine-tuning** these models on custom data (e.g. a specific API or codebase) is generally feasible with low-rank adaptation (LoRA) or other parameter-efficient techniques, especially for the smaller models. The larger models (70B, 100B+) can be fine-tuned with sufficient GPU resources or via distributed training. Notably, **Mistral 123B** has been shown to accept QLoRA fine-tunes even on single machines with enough RAM (one report achieved ~40 tokens/sec during LoRA training on an Apple M2 Ultra for Mistral-123B) ([Awni Hannun on X: "QLoRA fine-tuning Mistral Large 123B (dense ...](https://x.com/awnihannun/status/1816164492432310302#:~:text=Awni%20Hannun%20on%20X%3A%20,lm%20LoRA%20docs%20https%3A%2F%2Ft.co%2FiWjNfFv6TM)). Similarly, researchers have fine-tuned CodeLlama 34B and 70B on specialized coding instructions using 8×A100 GPUs or smaller hardware with gradient checkpointing.

One thing to consider is that **Mixture-of-Experts models** (like DeepSeek v2) may require custom training code to fine-tune, as you have to update expert weights and gating – but the DeepSeek team has provided their DeepSeekMoE framework ([GitHub - deepseek-ai/DeepSeek-Coder-V2: DeepSeek-Coder-V2: Breaking the Barrier of Closed-Source Models in Code Intelligence](https://github.com/deepseek-ai/DeepSeek-Coder-V2#:~:text=We%20present%20DeepSeek,V2%20expands%20its%20support%20for)), and one could apply LoRA to MoE layers as well. For most users, leveraging the existing instruction-tuned checkpoints is the easiest route, and only doing further fine-tuning if needed for your domain (for instance, fine-tuning CodeGemma-7B on your company’s code style).

**Multi-turn instruction** fine-tuning is already well-handled in these models. Many (Qwen, CodeLlama, DeepSeek instruct) were trained with chat transcripts or role prompt formats, making them effective in an IDE agent scenario (where the model can ask clarifying questions, suggest changes, etc.). If you need a model to adhere to a specific style or formatting (say, always producing code with certain comments), a lightweight fine-tune or even prompt engineering with these instruction models can usually get you there. The key is that all are **open** to modification: you have full weight access, so you can apply any standard fine-tuning library (like Hugging Face Transformers with PEFT/LoRA) to customize them.

It’s worth noting that the **community has produced numerous fine-tuned variants** of these base models as well. For instance, there are “WizardCoder” models based on CodeLlama that inject more conversational ability, or specialized Qwen coder variants for certain languages. While the question focuses on base models, keep in mind this ecosystem – you might find an existing fine-tune that fits your needs (saving you time), or you can contribute your own.

## Supported Programming Languages
One of the big differences among these models is the **range of programming languages** they can handle effectively. All models support Python very well, as it’s the most common language in training data and benchmarks. The distinctions arise with less common languages or specific domains.

- **DeepSeek Coder v2** expanded its support dramatically from an already large set (86 languages) to **338 programming languages** ([GitHub - deepseek-ai/DeepSeek-Coder-V2: DeepSeek-Coder-V2: Breaking the Barrier of Closed-Source Models in Code Intelligence](https://github.com/deepseek-ai/DeepSeek-Coder-V2#:~:text=the%20coding%20and%20mathematical%20reasoning,length%20from%2016K%20to%20128K)). This likely covers just about every language on GitHub – including obscure ones. In practice, this means DeepSeek v2 can assist with everything from mainstream languages (Python, Java, C++) to esoteric ones (Brainfuck, VHDL, you name it). The original DeepSeek models (1.3B–33B) were bilingual in natural language (English and Chinese) ([deepseek-ai/deepseek-coder-33b-instruct · Hugging Face](https://huggingface.co/deepseek-ai/deepseek-coder-33b-instruct#:~:text=Deepseek%20Coder%20is%20composed%20of,programming%20languages%20and%20various%20benchmarks)), so they can understand prompts or comments in Chinese and English. For programming languages, the original models were evaluated on MultiPL-E (which includes Python, JavaScript, Java, C#, etc.) and showed top-tier performance ([blog/codegemma.md at main · huggingface/blog · GitHub](https://github.com/huggingface/blog/blob/main/codegemma.md#:~:text=CodeGemma,leaderboard%20or%20some%20metrics%20below)), indicating strong multi-language code support as well. So DeepSeek is a good choice if you have a multilingual coding environment (both in terms of human language and programming language).

- **Qwen 2.5 Coder** supports **40+ programming languages** out-of-the-box ([Qwen2.5-Coder Series: Powerful, Diverse, Practical. | Qwen](https://qwenlm.github.io/blog/qwen2.5-coder-family/#:~:text=,training%20phase)). The team specifically mentions good performance in languages like **Haskell, Racket,** and presumably other functional or less-common languages, thanks to careful data balancing ([Qwen2.5-Coder Series: Powerful, Diverse, Practical. | Qwen](https://qwenlm.github.io/blog/qwen2.5-coder-family/#:~:text=,training%20phase)). This breadth covers all popular languages (Python, C-family, JavaScript, Java, PHP, Ruby, etc.) and many niche ones. Qwen models also retain general multilingual abilities from their base (for example, Qwen is known to support English and Chinese fluently, and other human languages to some extent ([QwenLM/Qwen2.5 - GitHub](https://github.com/QwenLM/Qwen2.5#:~:text=QwenLM%2FQwen2.5%20,including%20Chinese%2C%20English%2C%20French))). So you could prompt Qwen in Chinese to write code, and it should handle that well. The combination of **multi-natural-language and multi-programming-language** support makes Qwen quite versatile as a coding assistant globally.

- **CodeLlama** was trained on the BigCode/The Stack dataset, which includes a wide array of languages, though Meta didn’t enumerate them in the announcement. They did highlight that all CodeLlama models **outperformed previous open models on MultiPL-E** (which involves Python, C++, Java, JavaScript, Rust, Go, etc.) ([[2308.12950] Code Llama: Open Foundation Models for Code](https://arxiv.org/abs/2308.12950#:~:text=100k%20tokens,both%20research%20and%20commercial%20use)). Notably, they released a **Python-specialized variant** which boosts Python performance at some expense to other languages. For most users, the generic CodeLlama or CodeLlama-Instruct is suitable for multi-language coding. It can handle all major languages and then some, but it might not be as deeply trained on very niche languages compared to DeepSeek or Codestral. CodeLlama does support **code infilling**, which is language-agnostic – meaning you can use it to complete code in the middle of a file for any language it knows, given the proper syntax.

- **CodeGemma** was trained on “primarily English language data, mathematics, and code” ([blog/codegemma.md at main · huggingface/blog · GitHub](https://github.com/huggingface/blog/blob/main/codegemma.md#:~:text=CodeGemma%20is%20a%20family%20of,for%20code%20completion%20and%20generation)). It doesn’t explicitly list the languages, but given it’s a Google model, we can assume it saw a comprehensive set of programming languages (likely the same Stack dataset or an internal Google code dataset). The emphasis in their paper is on **robustness across Python, Java, JavaScript, C++** (they mention these in evaluation) ([blog/codegemma.md at main · huggingface/blog · GitHub](https://github.com/huggingface/blog/blob/main/codegemma.md#:~:text=CodeGemma,leaderboard%20or%20some%20metrics%20below)). They also note CodeGemma-7B performed best among 7Bs on **GSM8K math** problems ([blog/codegemma.md at main · huggingface/blog · GitHub](https://github.com/huggingface/blog/blob/main/codegemma.md#:~:text=goes%20for%20the%20evaluation%20of,leaderboard%20or%20some%20metrics%20below)), showing it has strong logical reasoning as well as coding. The **2B model** is aimed at infilling and completion, which presumably works for any language it learned (it was 100% code infilling-trained ([blog/codegemma.md at main · huggingface/blog · GitHub](https://github.com/huggingface/blog/blob/main/codegemma.md#:~:text=CodeGemma%20is%20a%20family%20of,for%20code%20completion%20and%20generation)), possibly on multiple languages). So CodeGemma should be competent in popular languages (Python, JS, Java, C/C++). It may not explicitly support as many languages as Codestral or DeepSeek, but for typical use (backend scripts, algorithmic problems, etc.) it’s on par. Since it was built on **Gemma**, which in turn might share roots with PaLM 2, it’s likely multilingual to a degree in natural language understanding but the focus is coding tasks.

- **Command R+ / A** (Cohere’s model) is a general LLM, not exclusively a code model. It’s been trained on a mix of internet data (so it knows code, but also a lot of prose). Cohere hasn’t published a list of programming languages, but one can infer it’s proficient in mainstream ones (it likely saw GitHub and other code in training). Given its focus on “agentic tasks” and RAG, it probably handles things like pseudo-code, Bash scripting, SQL (for which RAG could be used), etc., as part of its toolkit. **Long context** ability means it can ingest large code files or multiple files (even a whole repo up to 256K tokens) and reason about them, which is a unique advantage for languages where context matters (like reading an entire codebase across files). If your use case involves mixing natural language and code (e.g., a conversation about code with references to documentation), Command R+ will excel since it’s a conversational model at heart with coding skills added. For purely coding in a less common language, it may not be as specialized as the code-specific models.

- **Codestral 22B** is explicitly **fluent in 80+ programming languages** ([Codestral | Mistral AI](https://mistral.ai/news/codestral#:~:text=A%20model%20fluent%20in%2080%2B,programming%20languages)). Mistral AI states it performs well even in languages like **Swift and Fortran** beyond the usual suspects ([Codestral | Mistral AI](https://mistral.ai/news/codestral#:~:text=A%20model%20fluent%20in%2080%2B,programming%20languages)). This breadth is one of Codestral’s selling points: it can help with just about any language a developer might encounter, making it a great all-purpose coding assistant. Additionally, Codestral’s training likely included many **shell and query languages** (the mention of Bash and SQL is there ([Codestral | Mistral AI](https://mistral.ai/news/codestral#:~:text=A%20model%20fluent%20in%2080%2B,programming%20languages)) ([Codestral | Mistral AI](https://mistral.ai/news/codestral#:~:text=Repository))), so it can generate code for database queries (it did well on the Spider SQL benchmark) and CLI scripts. If you work in a polyglot code environment, Codestral’s broad knowledge is advantageous.

- **Mistral Large 123B** (Instruct) presumably inherits the multilingual coding ability of Mistral’s training data. Mistral 7B was trained on the Stack and demonstrated strong results across languages. The 123B should only improve on that with more capacity. The Ollama card for Mistral Large mentions “support for dozens of languages” ([mistral-large:123b/license](https://ollama.com/library/mistral-large:123b/blobs/9bac90050d38#:~:text=Mistral%20Large%202%20is%20Mistral%27s,support%20for%20dozens%20of%20languages)), indicating it’s not limited to a few. It can likely handle code and pseudocode in many languages, plus its **128K context** means it could take in code mixed with documentation in different languages (imagine feeding it code with comments in French or Spanish – it could deal with that context). However, since Mistral 123B is a general model, it might not have undergone a code-specific fine-tune, so you may sometimes need to guide it more in languages with less presence in the training data.

In summary, if you need a model that can code in a **specific less-common programming language**, **DeepSeek v2 or Codestral** are excellent choices due to their huge language coverage. **Qwen and DeepSeek original** also cover a broad range, plus can understand prompts in multiple human languages. For a **mix of coding and conversation across languages**, something like **Command R+** (with its long context and tool use) or **Mistral-123B** could be useful. **Most models handle mainstream languages** (Python, Java, C, JS, etc.) very well, so for typical use cases that stick to common languages, all will do the job. It’s only when you venture into, say, writing code in **MATLAB, COBOL or niche DSLs** that the differences emerge – at which point the models explicitly trained on many languages (Codestral, DeepSeek) shine.

## Pricing and Licensing Considerations
One important practical factor is the **licensing and cost** associated with each model. All the models discussed are “local” and open-weight (no per-token API fees), but the ability to use them commercially and their distribution terms vary:

- **DeepSeek Coder (original & v2)** – Released by DeepSeek AI under a model license that **allows commercial use** ([deepseek-ai/deepseek-coder-33b-instruct · Hugging Face](https://huggingface.co/deepseek-ai/deepseek-coder-33b-instruct#:~:text=4)). The code is MIT-licensed and the model weights have a separate license (which, per DeepSeek, supports commercial applications) ([deepseek-ai/deepseek-coder-33b-instruct · Hugging Face](https://huggingface.co/deepseek-ai/deepseek-coder-33b-instruct#:~:text=4)). This means you can integrate DeepSeek models into a product or service without paying royalties, which is a big plus for startups or companies. The **236B model**, however, is almost impractical to self-host for most due to hardware costs – instead, DeepSeek offers an API (with pricing around **$0.14 per 1M input tokens** on their platform, and $0.28 per 1M output) ([DeepSeek-Coder-V2: Breaking the Barrier of Closed-Source Models ...](https://www.reddit.com/r/LocalLLaMA/comments/1dhx449/deepseekcoderv2_breaking_the_barrier_of/#:~:text=DeepSeek,28)). So if you want 236B-level performance, you might opt to pay for their hosted solution instead of buying enormous GPUs. The 16B model you can run yourself relatively cheaply (just the one-time hardware or cloud instance cost). In short: *licensing won’t be a cost issue for DeepSeek*, but compute might be.

- **Qwen 2.5 Coder** – **Apache License 2.0** on the weights ([qwen2.5-coder-7b-instruct Model by Qwen - NVIDIA NIM APIs](https://build.nvidia.com/qwen/qwen2_5-coder-7b-instruct/modelcard#:~:text=qwen2.5,0%20License)), which is very permissive. You can use Qwen models in commercial products, modify them, and even redistribute them, with no special restrictions. This is as open as it gets (aside from the need to attribute if you redistribute). Alibaba has essentially made these free to use. There’s no official paid service for Qwen coder as of writing; it’s meant for local deployment. So your costs are purely infrastructure (GPUs, etc.). This makes Qwen a great choice if you want **zero licensing headaches** – you can build it into your company’s internal tools or even customer-facing apps freely.

- **CodeLlama** – Released under Meta’s **community license**, which, while not “open source” by OSI definition, **is free for research and commercial use** for most users ([[2308.12950] Code Llama: Open Foundation Models for Code](https://arxiv.org/abs/2308.12950#:~:text=art%20performance%20among%20open%20models,both%20research%20and%20commercial%20use)). The main caveat in Llama 2 license was that if the end users of your product exceed 700 million monthly, you’d need special permission. For the vast majority of projects, that’s a non-issue. Also you must avoid certain use cases (like violence, etc., per the Acceptable Use Policy). Essentially, CodeLlama is **free to use commercially** for small and large companies alike, with reasonable AUP restrictions. No royalties or payments needed – Meta provided the weights openly. So again, cost is just hardware. Many cloud providers (AWS, GCP) even host CodeLlama 70B for on-demand use, possibly with their own pricing, but you always have the self-hosted option.

- **CodeGemma** – The weights are open-access but require agreeing to Google’s Gemma Terms. Those terms **“permit responsible commercial usage and distribution for all organizations, regardless of size.”** ([Gemma: Introducing new state-of-the-art open models](https://blog.google/technology/developers/gemma-open-models/#:~:text=Gemma%3A%20Introducing%20new%20state,all%20organizations%2C%20regardless%20of%20size)) This is actually more permissive in one sense than Llama’s (no user-count restriction). Essentially, Google is allowing commercial use, but likely with clauses similar to Meta’s (no misuse, etc.). There’s no cost to get the model (just accept on Hugging Face or Kaggle), and you won’t owe Google anything for using it. It’s a strong offering because you get Google-researched quality without a price tag or heavy limitations. The **only slight hurdle** is the click-through license – which is fine for most, but if you want truly Apache-style, Qwen would be the alternative. As for pricing: Google could integrate Gemma models into GCP services eventually, but using CodeGemma 7B/2B locally is cost-free aside from compute.

- **Command R+ (111B)** – Cohere’s Command models are under a custom non-commercial license when weights are released via their research arm (Cohere For AI). Indeed, **Command R+ was not allowed for commercial use** by default ([C4AI Command A 111B : r/LocalLLaMA](https://www.reddit.com/r/LocalLLaMA/comments/1jabin6/c4ai_command_a_111b/#:~:text=%E2%80%A2)), and the new Command A 111B appears to follow suit (community comments noted “license is meh (non-commercial)” ([C4AI Command A 111B : r/LocalLLaMA](https://www.reddit.com/r/LocalLLaMA/comments/1jabin6/c4ai_command_a_111b/#:~:text=%E2%80%A2))). This means you can **experiment and research** with R+/A, but you **cannot integrate it into a revenue-generating product or service** without a separate agreement. Cohere is a for-profit providing API access to their models, so presumably they want businesses to pay for their hosted model (which is priced per token, similar to OpenAI’s pricing). If you want to self-host Command R+ for a commercial application, you’d have to negotiate with Cohere – which likely involves a fee or contract. Therefore, from a cost perspective, using Command R+ in production could be expensive or simply off-limits. For personal or academic use, it’s free. Hardware-wise, to run it yourself you’d need those two high-end GPUs or a multi-node setup, which is also a significant cost. So, consider Command R+ more of a **research/demo model** unless you’re prepared to engage with Cohere’s enterprise offerings.

- **Codestral 22B** – Mistral AI released Codestral under the **Mistral AI Non-Production License** ([Codestral | Mistral AI](https://mistral.ai/news/codestral#:~:text=Codestral%20is%20a%2022B%20open,can%20be%20downloaded%20on%20HuggingFace)). This **allows research, testing, and internal evaluation**, but prohibits using the model or its outputs in any production or revenue-generating scenario without a commercial license. They explicitly say you can **contact them for a commercial license** ([Codestral | Mistral AI](https://mistral.ai/news/codestral#:~:text=Codestral%20is%20a%2022B%20open,can%20be%20downloaded%20on%20HuggingFace)). This likely involves a fee or partnership. In essence, Mistral’s strategy is to showcase a great model openly, then monetize it via licensing deals. If you’re an individual developer or researcher, you get to use Codestral for free on your own machine. If you’re a company that wants to deploy it in your product (e.g., as part of an IDE or a coding assistant service), you’d have to pay Mistral. The cost isn’t public; it’d be negotiated. If you cannot or don’t want to deal with that, you might stick to truly open models instead (like CodeLlama or Qwen) for commercial needs. On the compute side, Codestral being 22B means it’s actually quite cheap to run relative to bigger models – it’s feasible on a single GPU. So the main cost consideration is the **licensing** if commercial.

- **Mistral Large 123B** – Similar story to Codestral. It’s under the **Mistral AI Research License** ([mistral-large:123b/license](https://ollama.com/library/mistral-large:123b/blobs/9bac90050d38#:~:text=Mistral%20AI%20Research%20License)), which is essentially non-commercial. You can play with it, fine-tune it, even distribute it (with the same license attached), but if you want to use it “for any purpose not expressly authorized” (i.e., likely any profit or user-facing service), you must get a license from Mistral AI ([mistral-large:123b/license](https://ollama.com/library/mistral-large:123b/blobs/9bac90050d38#:~:text=Mistral%20AI%20Research%20License)). This is understandable given the value of such a large model. So, for a company, using Mistral 123B in production would involve contacting Mistral (and presumably some $$$ or a cloud contract when they offer it as a service). For personal use, it’s free. As for *pricing*, since it’s not offered via API yet (to public knowledge), the “price” is essentially the GPU cost – which is high if you try to run 123B (maybe you rent an expensive cloud VM at a few dollars an hour). In many cases, if a company needed that power, they might find it more cost-effective to pay for an optimized API model (like GPT-4) rather than running a 123B locally, unless they have very specific privacy or integration needs.

To summarize the licensing landscape: **Qwen 2.5 Coder and CodeLlama** (and CodeGemma, effectively) are the most **business-friendly open models**, with no royalties and broad usage rights ([qwen2.5-coder-7b-instruct Model by Qwen - NVIDIA NIM APIs](https://build.nvidia.com/qwen/qwen2_5-coder-7b-instruct/modelcard#:~:text=qwen2.5,0%20License)) ([[2308.12950] Code Llama: Open Foundation Models for Code](https://arxiv.org/abs/2308.12950#:~:text=art%20performance%20among%20open%20models,both%20research%20and%20commercial%20use)). **DeepSeek** models also appear to allow commercial use freely ([deepseek-ai/deepseek-coder-33b-instruct · Hugging Face](https://huggingface.co/deepseek-ai/deepseek-coder-33b-instruct#:~:text=4)), which is great for adoption. On the other hand, **Cohere’s Command** and **Mistral’s models (Codestral, 123B)** are in the category of **“open weights, but restricted use”**, meaning they are fantastic for research or internal prototyping, but you need to **pay or negotiate for commercial deployment**. This isn’t to say you shouldn’t consider them – if their performance fits a niche need, perhaps that cost is justified. But if you’re just looking to avoid any licensing entanglements, you’d lean towards the truly open ones. Lastly, **running costs** (hardware) scale with model size – smaller models are essentially free to run on consumer hardware, whereas the largest might require cloud instances that could run into hundreds or thousands of dollars per month. Always weigh whether a slight boost in code accuracy from a bigger model is worth the extra infrastructure expense, especially since open models have no usage fee per se.

## Strengths and Weaknesses by Model
Below is a summary of each model (or family) highlighting their **strengths, weaknesses, and ideal use cases**:

- **DeepSeek Coder v2 (16B & 236B)** – *Strengths:* Outstanding code and math performance (comparable to GPT-4) ([GitHub - deepseek-ai/DeepSeek-Coder-V2: DeepSeek-Coder-V2: Breaking the Barrier of Closed-Source Models in Code Intelligence](https://github.com/deepseek-ai/DeepSeek-Coder-V2#:~:text=We%20present%20DeepSeek,V2%20expands%20its%20support%20for)). Extremely long context (128k) for reading large projects. Supports an unparalleled 338 programming languages ([GitHub - deepseek-ai/DeepSeek-Coder-V2: DeepSeek-Coder-V2: Breaking the Barrier of Closed-Source Models in Code Intelligence](https://github.com/deepseek-ai/DeepSeek-Coder-V2#:~:text=the%20coding%20and%20mathematical%20reasoning,length%20from%2016K%20to%20128K)). MoE architecture offers high efficiency (21B active params) for its size ([GitHub - deepseek-ai/DeepSeek-Coder-V2: DeepSeek-Coder-V2: Breaking the Barrier of Closed-Source Models in Code Intelligence](https://github.com/deepseek-ai/DeepSeek-Coder-V2#:~:text=2)). *Weaknesses:* The 236B model is resource-heavy to deploy (practically requires a multi-GPU server or using DeepSeek’s paid API). MoE models can be more complex to fine-tune or serve than standard ones. *Optimal Use:* If you require **top-tier coding accuracy** (enterprise-level code generation or complex competitive programming problems) and have the means (or willingness to use their service), DeepSeek v2 is ideal. The 16B version is great when you need strong performance on a single GPU – it can match or beat much larger dense models on code tasks, making it a **high-value choice for a self-hosted coding assistant**.

- **DeepSeek Coder (33B, 6.7B, 1.3B)** – *Strengths:* Proven state-of-the-art results among open models at their release ([deepseek-ai/deepseek-coder-33b-instruct · Hugging Face](https://huggingface.co/deepseek-ai/deepseek-coder-33b-instruct#:~:text=capabilities%2C%20Deepseek%20Coder%20achieves%20state,programming%20languages%20and%20various%20benchmarks)) – the 33B especially is still very competitive, and the 6.7B punches above its weight on code benchmarks ([blog/codegemma.md at main · huggingface/blog · GitHub](https://github.com/huggingface/blog/blob/main/codegemma.md#:~:text=CodeGemma,leaderboard%20or%20some%20metrics%20below)). 16K context window enables handling larger files than many other models of similar size. Bilingual training (English/Chinese) adds versatility for non-English prompts. Available in multiple sizes to suit different hardware. *Weaknesses:* Slightly older architecture (Llama2-based with some tweaks), so newest models like Qwen or CodeLlama might have better training or data. The smallest 1.3B is quite limited in capability (only suitable for very simple tasks or as a proof of concept). *Optimal Use:* **33B** – a sweet spot if you want high coding proficiency without going to 70B scale; great for an on-premises coding assistant that can handle most tasks. **6.7B** – good for lightweight scenarios, maybe running on a laptop or low-end GPU, where you still want reasonable code completion and can tolerate some mistakes. **1.3B** – only consider for ultra-low-end devices or perhaps fine-tuning experiments, knowing its outputs will be far less reliable.

- **Qwen 2.5 Coder (32B, 14B, 7B)** – *Strengths:* Excellent code generation and reasoning skills, especially the 32B which is arguably the **best open model per parameter** right now ([Qwen2.5-Coder Series: Powerful, Diverse, Practical. | Qwen](https://qwenlm.github.io/blog/qwen2.5-coder-family/#:~:text=%2A%20Powerful%3A%20Qwen2.5,Coder%20in%20two%20scenarios)). Competitive with models 2-3× its size. Multilingual both in understanding queries and in programming languages (40+ langs) ([Qwen2.5-Coder Series: Powerful, Diverse, Practical. | Qwen](https://qwenlm.github.io/blog/qwen2.5-coder-family/#:~:text=,training%20phase)). Apache 2.0 license means you can use it anywhere freely. Also, Qwen models are known to be **well-behaved** in following instructions and not overly verbose unless needed. *Weaknesses:* The 14B model’s anomaly on some benchmarks suggests it might not be as well-rounded as the 7B or 32B ([Qwen 2.5 Coder 14b is worse than 7b on several benchmarks in the technical report - weird! : r/LocalLLaMA](https://www.reddit.com/r/LocalLLaMA/comments/1gpriif/qwen_25_coder_14b_is_worse_than_7b_on_several/#:~:text=https%3A%2F%2Farxiv)) – perhaps a minor training issue, but in practice it’s still solid. The 32B, while smaller than giants, still needs a decent GPU setup (it’s not as accessible as say a 13B model). As with any new release, community support (e.g., fine-tunes, tooling) for Qwen is growing but slightly less mature than Meta’s ecosystem. *Optimal Use:* **Qwen-32B** is an excellent choice for a company that wants near state-of-the-art coding help on a self-hosted machine – for example, integrating into a code review tool or documentation generator. It’s big but manageable, and free to use. **Qwen-7B** shines in resource-constrained uses – think of embedding it in a local VSCode extension for autocompletion or using it on a mobile device for on-the-go code fixes (it’s surprisingly capable for its size, thanks to training on trillions of tokens). If trying Qwen-14B, you’d use it when 7B isn’t enough, but 32B is too heavy – it should perform well, just keep an eye on any edge cases where it might underperform and consider testing 7B vs 14B for your specific tasks.

- **CodeLlama (70B, 34B, 13B, 7B)** – *Strengths:* Backed by Meta’s Llama 2, these models are robust and have undergone extensive fine-tuning. They handle instruction following in a coding context very naturally (especially the instruct versions). They support infilling, which many models do not, enabling advanced IDE features (fill in the middle, refine existing code). The **70B** and **34B** versions provide strong performance on code and can also carry on a conversation about code (useful for explaining code or guiding a user). **7B and 13B** are efficient – 7B can even beat older 70B models on code tasks ([[2308.12950] Code Llama: Open Foundation Models for Code](https://arxiv.org/abs/2308.12950#:~:text=100k%20tokens,both%20research%20and%20commercial%20use)), making it a great lightweight coder for basic needs. Licensing is fairly open (commercial use allowed with few restrictions), which is enterprise-friendly. *Weaknesses:* CodeLlama’s training data, while large, might not be as specialized or up-to-date as some newer releases – for instance, it might have seen less of certain niche languages or less refined instruction data compared to Qwen or DeepSeek. The 4K context on base models is lower than some competitors (though in practice they handle up to 16K). Also, since Meta moved on to Llama 3, CodeLlama (based on Llama2) might not incorporate some latest architecture improvements. *Optimal Use:* **CodeLlama-70B** – great for a high-end coding assistant that you can deploy internally, especially if you value stability and have already used Llama 2 models. It’s a bit easier to run than 100B models and still reliable. **CodeLlama-13B** – a balanced choice when you need a capable model on a single GPU (fits in ~16GB with 8-bit). Many use the 13B in editors for code completion and get good results, especially with the Python-tuned variant for Python-centric work. **CodeLlama-7B** – ideal for personal projects or as an assistant on smaller devices; it can even be fine-tuned into a chatbot that talks about coding. Overall, choose CodeLlama if you want a well-rounded coder with **Meta’s support and a permissive license**, and you don’t necessarily need the absolute newest model.

- **CodeGemma (7B & 2B)** – *Strengths:* Extremely efficient models from Google. The 7B model is very good at not just coding, but also understanding instructions and performing reasoning steps (given its training mix) ([[2406.11409] CodeGemma: Open Code Models Based on Gemma](https://arxiv.org/abs/2406.11409#:~:text=code%20models%20built%20on%20top,sensitive%20settings)). It’s resilient in conversation – you can chat with it about coding problems and it won’t easily get confused. It was shown to be the best 7B on some math + code benchmarks ([blog/codegemma.md at main · huggingface/blog · GitHub](https://github.com/huggingface/blog/blob/main/codegemma.md#:~:text=CodeGemma,leaderboard%20or%20some%20metrics%20below)), so if your coding tasks involve analytical thinking (like writing a complex algorithm), it has an edge. The **2B** model’s strength is speed – it’s designed for completion, so it can inject code suggestions almost instantaneously, perfect for live coding support with minimal lag. Also, both models have **8K context**, slightly more than many small models, which helps when files are a bit longer. *Weaknesses:* As only 7B and 2B, they obviously can’t compete with 30B+ models on very complex coding tasks – they might fail more often on tricky LeetCode hards or large codebase comprehension. Also the fact that they’re relatively new means the community is still building tooling (though integration in Transformers is there). The license requires a click-through, which is fine, but some may prefer not having that layer. *Optimal Use:* **CodeGemma-2B** is fantastic for embedding into resource-limited environments: think of a **code auto-complete in a browser-based IDE** or a GitHub Copilot-like extension that runs locally. It gives you quick suggestions and can even do basic generation, all without offloading to a server. **CodeGemma-7B** is a good choice for a **coding chatbot or tutor** – for example, a Stack Overflow assistant that explains code or a Telegram bot that answers programming questions. It’s also viable for on-device use (some have run 7B models on smartphones or single-board computers for fun). Basically, if you want **Google-quality code AI on a budget**, CodeGemma is a prime candidate. And since it’s open for commercial use, a small tech company could integrate the 7B to power a user-facing coding help feature without licensing fees (just ensure the usage abides by Google’s terms of use for Gemma).

- **Command R+ / Command A (Cohere 111B)** – *Strengths:* **Very large, generalist model** that can handle complex dialogue and code. It’s excellent at multi-step reasoning – for instance, if you need the model to plan out a coding project or debug through conversation, it can do that, maintaining coherence over long exchanges. Its support for tools (via RAG and APIs) means it can integrate knowledge retrieval with coding, potentially making it more powerful in a connected environment. The new Command A’s **256K context** is industry-leading ([Command A — Cohere](https://docs.cohere.com/v2/docs/command-a#:~:text=enterprise%20tasks%20including%20tool%20use%2C,2024)); you can literally paste in huge codebases or lengthy logs and it can work with that, which others cannot to the same extent. *Weaknesses:* The **non-commercial license** is the big one – it’s not an “out-of-the-box” solution for products without dealing with Cohere. Also, being a general model, it wasn’t purely optimized for code. It may not be as straightforward as code-specific models in things like producing only code (it might include more explanatory text unless prompted carefully). Running it is expensive and not really feasible on typical consumer hardware. *Optimal Use:* **Research and prototyping**. If you’re a researcher looking to test the limits of long-context coding (e.g., feeding entire repositories and asking high-level questions), Command A is unmatched in context length. It’s also a great testbed for **agentic coding tasks** – like having the model use a compiler or run tests in a loop (Cohere demonstrated tool use with Command). For a company, you might use R+ internally to see how well a super-powerful model assists your developers, then decide if it’s worth pursuing a deal. It could also be used to generate large volumes of documentation or comments for a codebase, given you can feed everything into it. But for day-to-day smaller coding tasks, its extra capabilities might be overkill compared to simpler models.

- **Codestral 22B** – *Strengths:* **High performance with moderate size** – it often beats models 2–3x its size ([Codestral | Mistral AI](https://mistral.ai/news/codestral#:~:text=Performance,previous%20models%20used%20for%20coding)), thanks to focused training. It’s proficient in a **wide array of languages** and excels in long-range code completion due to 32K context ([Codestral | Mistral AI](https://mistral.ai/news/codestral#:~:text=Figure%201%3A%20With%20its%20larger,range%20eval%20for%20code%20generation)). It also has a dual ability to handle instructions and completion, meaning you can chat with it or use it in an IDE for filler – very flexible. Another subtle strength: since it’s from Mistral (who also made a strong 7B), the model likely benefits from some architectural improvements and training techniques that yield better efficiency per parameter. *Weaknesses:* The license – you can’t use it commercially without arrangements ([Codestral | Mistral AI](https://mistral.ai/news/codestral#:~:text=Codestral%20is%20a%2022B%20open,can%20be%20downloaded%20on%20HuggingFace)). Also, being 22B, it’s a bit of a niche size – those with 24GB GPUs might have instead gone for 34B or 32B models; those with only 16GB might prefer 13B. So it sits in between, which is not really a weakness of the model itself, but in terms of community adoption, there might be slightly fewer third-party fine-tunes or quantizations readily available compared to, say, 13B or 30B models. *Optimal Use:* If you’re okay with the non-commercial restriction, Codestral is **excellent for personal coding projects or internal company tooling**. For example, a dev team could use it internally to get code suggestions in many languages (keeping it in research use) to speed up development. Its ability to handle **large files** makes it useful for tasks like completing a function that relies on context from far above in a file, or even generating code given a lengthy spec as input. Also, if you participate in programming contests or evaluations that allow AI assistance, Codestral could be a secret weapon due to its strong performance and fast inference (relative to bigger models).

- **Mistral Large 123B** – *Strengths:* **All-around powerhouse** – it has huge knowledge and can perform not just coding but any reasoning task you throw at it. For coding, it can combine its knowledge of frameworks, algorithms, and even some documentation content (from training) to produce very insightful outputs. The **128K context** is a boon for working with large code bases or multiple files – you could feed in several source files and ask it to find bugs or suggest improvements across them. Mistral is also known for being a bit more **uncensored/out-of-the-box** (depending on the instruct tuning) ([C4AI Command A 111B : r/LocalLLaMA](https://www.reddit.com/r/LocalLLaMA/comments/1jabin6/c4ai_command_a_111b/#:~:text=%E2%80%A2)), which might help if you want a model that doesn’t refuse tasks – it will straightforwardly try to comply, which in coding means it won’t overly question your prompt (sometimes alignment can cause models to lecture rather than just give code – Mistral likely avoids that). *Weaknesses:* Again, license limitations for commercial use ([mistral-large:123b/license](https://ollama.com/library/mistral-large:123b/blobs/9bac90050d38#:~:text=Mistral%20AI%20Research%20License)). Additionally, the sheer size means latency is high; it’s not suitable for real-time suggestion in an editor unless you have a lot of compute. And as a general model, if not explicitly fine-tuned for code instructions, it might need more careful prompting (like using a system message that says it’s a coding assistant). The model’s weight also means it’s harder for others to fine-tune and experiment with, so you might not see as many community-driven code-specialized versions of Mistral-123B (unlike smaller models where hobbyists create Python-specific fine-tunes, etc.). *Optimal Use:* **AI research labs and advanced development teams** might use Mistral 123B to push the envelope – e.g., exploring AI pair programming where the model has the entire project context. If you have access to a powerful computing cluster and need a model that can handle highly complex coding tasks (maybe synthesizing code from a very high-level problem description or performing code translation between languages at scale), this model can do it, albeit slowly. It’s also a good choice if you want one model that can do coding *and* everything else (writing docs, reasoning about requirements, answering general questions) in an integrated assistant – Mistral can be that one brain for all tasks due to its generalist training.

## Recommendations by Use Case
Finally, to match these models to different **coding needs**, here are some recommendations:

- **For lightweight local coding assistance (low VRAM or CPU-only):** Use **CodeGemma-2B** or **DeepSeek Coder 1.3B**. CodeGemma-2B in particular is optimized for code completion and will give the snappiest responses when resources are minimal. These are perfect for integrating into editors on a laptop or even running on a Raspberry Pi for fun. Expect decent autocomplete and small function generation, but not miracles on complex logic.

- **For a balance of performance and speed on a single GPU:** Consider models in the 7B–16B range. **DeepSeek Coder 6.7B** is great if you want the best accuracy at ~7B size (especially for Python) ([blog/codegemma.md at main · huggingface/blog · GitHub](https://github.com/huggingface/blog/blob/main/codegemma.md#:~:text=CodeGemma,leaderboard%20or%20some%20metrics%20below)), while **Qwen 7B** and **CodeGemma 7B** offer more well-rounded abilities (Qwen for multi-language and robust instructions, CodeGemma for math+code reasoning). If you have around 16 GB VRAM, you could even step up to **CodeLlama-13B** or **Qwen 14B** for a bit more headroom. These will handle most everyday coding tasks (writing functions, simple scripts, etc.) with reasonable success and still run at interactive speeds (~10 tokens/sec). They’re also all fine for commercial projects (Qwen/CodeLlama are free to use), so they make a good starting point for a small company’s coding AI feature.

- **For the best open-code model on a high-end single machine:** **Qwen 2.5 Coder 32B** is a top recommendation. If you have a 24–48 GB GPU or a couple of smaller GPUs, Qwen-32B gives near state-of-the-art code generation without the hassle of restricted licenses. It’s a sweet spot where you get GPT-4-like abilities on code (not equal, but the closest among open models in mid-2024) ([Qwen2.5-Coder Series: Powerful, Diverse, Practical. | Qwen](https://qwenlm.github.io/blog/qwen2.5-coder-family/#:~:text=%2A%20Powerful%3A%20Qwen2.5,Coder%20in%20two%20scenarios)). Another option is **Codestral 22B** if your tasks involve a lot of different programming languages or very long files – it’s slightly less accurate than Qwen-32B in general, but still excellent and easier to run (plus that 32K context). However, remember Codestral’s license means internal use only unless you negotiate a deal. If licensing is a concern, stick to Qwen or **DeepSeek-33B** (DeepSeek-33B is older but still a champ and commercially allowed). In summary, for a powerful self-hosted coder on one server: **Qwen-32B for unrestricted use**, or **Codestral-22B/DeepSeek-33B** as alternatives (with the noted license difference).

- **For maximum accuracy (GPT-4-like coding) and you have enterprise resources:** **DeepSeek Coder v2 (236B)** is the choice if you want to truly rival closed-source models. It requires significant compute, so perhaps you’d run it on a multi-GPU rig or use DeepSeek’s own cloud endpoint (which, pricing considered, is far cheaper than an OpenAI API if you’re doing millions of tokens given their $0.14 per 1M input token rate) ([DeepSeek-Coder-V2: Breaking the Barrier of Closed-Source Models ...](https://www.reddit.com/r/LocalLLaMA/comments/1dhx449/deepseekcoderv2_breaking_the_barrier_of/#:~:text=DeepSeek,28)). This model is ideal when coding accuracy is mission-critical – for instance, generating correct solutions in an automated coding pipeline or tackling very hard competitive programming problems. If for some reason DeepSeek v2 236B is not accessible, the next best thing is to ensemble a few models or use **Mistral-123B or Command A 111B**. Mistral-123B will give you excellent quality (and you can run it if you have the hardware), and Command A, while non-commercial, could be tested to see what an aligned 100B+ model can do. But straightforwardly, DeepSeek v2 has essentially “broken the barrier” to closed-source quality in code tasks ([GitHub - deepseek-ai/DeepSeek-Coder-V2: DeepSeek-Coder-V2: Breaking the Barrier of Closed-Source Models in Code Intelligence](https://github.com/deepseek-ai/DeepSeek-Coder-V2#:~:text=We%20present%20DeepSeek,V2%20expands%20its%20support%20for)), so it gets the nod for the ultimate local coder (with the understanding that “local” in this case might be a data center machine!).

- **If you need very **long context** coding (analyzing big codebases, long logs, etc.):** Go for **Command A (111B, 256K)** or **Mistral-123B (128K)** if available in an instruct format. These can intake massive contexts that others can’t. For example, you can feed an entire code repository’s files into Command A and ask it questions – something not feasible with a 4K or 16K context model. For open-source options, note that **DeepSeek v2 models also have 128K context** ([GitHub - deepseek-ai/DeepSeek-Coder-V2: DeepSeek-Coder-V2: Breaking the Barrier of Closed-Source Models in Code Intelligence](https://github.com/deepseek-ai/DeepSeek-Coder-V2#:~:text=the%20coding%20and%20mathematical%20reasoning,length%20from%2016K%20to%20128K)), so DeepSeek-16B or 236B are options for long documents as well (and DeepSeek’s memory overhead for context might be lower due to MoE). **Codestral’s 32K** is another strong contender; 32K is enough for most single-file tasks and some multi-file scenarios. If you specifically need to handle tens of thousands of tokens of code, ensure the model you pick explicitly supports it (most others like CodeLlama and Qwen are 16K or 32K at most). In terms of recommendation: *for research or non-prod analysis*, it’s worth trying Command A or Mistral Large. *For a commercial setting*, you might use DeepSeek’s 128K via their service or stick with a 32K model like Qwen (some Qwen models had extended context variants) or Codestral if you can get a license.

- **For multi-language coding or specialized domains:** If you work with many programming languages or unusual ones (say you sometimes code in Rust, sometimes in Swift, sometimes in MATLAB), a model like **Codestral** or **DeepSeek v2** is most likely to have you covered due to their breadth of training. Qwen is also a good multi-language choice (40+ langs is plenty for most needs). For domain-specific code – for example, embedded C for microcontrollers, or academic languages like MATLAB/Octave – check model documentation for those keywords. DeepSeek’s 338 languages likely includes MATLAB and Verilog and so on, making it a safe bet. Also, consider fine-tuning a model on your domain if it’s niche. For instance, if you do a lot of **SQL** or **R** coding, you could fine-tune CodeLlama or Qwen on databases or R packages respectively. But out-of-the-box, **Codestral’s strong SQL performance (Spider benchmark leader)** and multi-language HumanEval scores indicate it’s a top pick for diverse language support. So the recommendation: **Codestral 22B** for broad programming language needs (research use), or **Qwen-32B** if you need a commercially usable model that’s pretty good at multi-language (though slightly fewer total languages than Codestral/DeepSeek).

- **For education and training (learning to code, explaining code):** A model that’s verbose and good at explanation is beneficial. **CodeLlama-Instruct** and **CodeGemma-7B-Instruct** are both friendly in explaining code in simple terms (CodeGemma was designed to follow instructions and likely has some RLHF that makes it align well with user intent). Also, Cohere’s Command models, being conversational, can do a great job role-playing a tutor (but licensing and size aside). For a student or teacher, a **7B–13B instruct model** is often sufficient to walk through code, comment code, or give hints on exercises. For example, **Llama2 13B Chat (not specifically a coder, but general)** can actually explain code pretty well, and CodeLlama would be even better at that. **Qwen-7B** has the advantage of understanding multiple languages, so it could explain code with bilingual comments (useful in regions where students might speak one language and code in English – Qwen can bridge that). In summary: choose a model that has an instruct tuning and is not too large (for cost reasons) – **CodeLlama-13B Instruct** or **CodeGemma-7B** would be my top picks to integrate into a learning platform or a coding course assistant.

In closing, the landscape of local coding models is rich – each model has its niche. If you prioritize **open licensing and community support**, lean towards **Meta’s CodeLlama or Alibaba’s Qwen**. If you need cutting-edge performance and have the hardware, **DeepSeek v2** or **Codestral/Mistral** will serve you best. And if you’re aiming somewhere in the middle (good performance, moderate compute), **the 7B–34B range** from various providers offers plenty of options to experiment with. Consider the specifics of your use case – context length needed, languages used, compute available, and any commercial constraints – and select the model that best aligns with those. With the rapid advances in this space, even smaller models are closing the gap to larger ones, so it’s a great time to mix and match and even try ensemble approaches (e.g., using a fast 7B for simple tasks and backing off to a 70B for hard tasks) to get the optimal blend of speed and accuracy in your coding applications. Each of the models above can be a valuable tool in the developer’s toolkit when used in the scenario that plays to its strengths. ([GitHub - deepseek-ai/DeepSeek-Coder-V2: DeepSeek-Coder-V2: Breaking the Barrier of Closed-Source Models in Code Intelligence](https://github.com/deepseek-ai/DeepSeek-Coder-V2#:~:text=CodeStral%2022B%2022B%2078,1))



## Model list

- **DeepSeek Coder v2** (236B & 16B)
- **DeepSeek Coder** (33B, 6.7B, 1.3B)
- **Qwen 2.5 Coder** (32B, 14B, 7B)
- **CodeLlama** (70B, 34B, 13B, 7B)
- **CodeGemma** (7B, 2B)
- **Command R+ (111B)**
- **Codestral (22B)**
- **Mistral Large (123B)**

## Key Comparison Factors

### Performance & Accuracy

| Model | Pass@1 (HumanEval) | Key Strengths |
|--------|-----------------|----------------|
| **DeepSeek Coder v2 (236B)** | ~90% | Near GPT-4-level coding, 338 languages, 128K context |
| **DeepSeek Coder v2 (16B)** | ~80% | Efficient MoE, strong performance for single GPU |
| **DeepSeek Coder (33B)** | ~77-79% | Solid accuracy, great alternative to larger models |
| **Qwen 2.5 Coder (32B)** | ~80% | Near GPT-4 parity, excellent multi-language support |
| **Qwen 2.5 Coder (7B)** | ~45-50% | Best small model, strong for lightweight tasks |
| **Codestral (22B)** | ~81% | High performance, 80+ programming languages |
| **Mistral Large (123B)** | ~75-80% | Excellent all-around model with long context |
| **CodeLlama (70B)** | ~67-68% | Strong performance, Meta-supported |
| **CodeGemma (7B)** | ~40-45% | Efficient, good at math and code reasoning |

### Speed & Memory Requirements

| Model | VRAM Requirement (16-bit) | Quantized (4-bit) | Tokens/sec |
|--------|-----------------|--------------|------------|
| **DeepSeek Coder v2 (236B)** | 1.5TB+ | ~128GB | 1-2 |
| **DeepSeek Coder v2 (16B)** | ~100GB | ~20GB | 8-10 |
| **Qwen 2.5 Coder (32B)** | ~60GB | ~20GB | 10+ |
| **Codestral (22B)** | ~40GB | ~16GB | 12-15 |
| **CodeLlama (70B)** | ~140GB | ~35GB | 5-8 |
| **CodeGemma (7B)** | ~14GB | ~4GB | 15-30 |

### Supported Programming Languages

- **DeepSeek Coder v2**: 338 languages (widest coverage)
- **Qwen 2.5 Coder**: 40+ languages, strong in functional programming
- **Codestral**: 80+ languages, strong in SQL and scripting
- **CodeLlama**: Broad coverage but less optimized for niche languages
- **CodeGemma**: Focused on Python, Java, C++, JavaScript

### Licensing & Pricing

| Model | License | Commercial Use |
|--------|--------|----------------|
| **DeepSeek Coder v2** | Open Model License | ✅ Free for commercial use |
| **Qwen 2.5 Coder** | Apache 2.0 | ✅ Free for commercial use |
| **CodeLlama** | Meta Community License | ✅ Free with restrictions |
| **CodeGemma** | Google Gemma Terms | ✅ Free for responsible use |
| **Command R+ (111B)** | Non-commercial | ❌ Not for commercial use |
| **Codestral (22B)** | Non-Production License | ❌ Requires a license for commercial use |
| **Mistral Large (123B)** | Research License | ❌ Non-commercial by default |

## Best Model by Use Case

- **Best for Top Accuracy (GPT-4-like performance)**: DeepSeek Coder v2 (236B) or Qwen 32B
- **Best for Balanced Performance on a Single GPU**: Qwen 32B or Codestral 22B
- **Best Small Model (Laptop-Friendly)**: CodeGemma 7B or DeepSeek 6.7B
- **Best Open-Source Model for Business**: Qwen 32B (Apache 2.0 license)
- **Best for Multi-Language Coding**: DeepSeek v2 (338 languages) or Codestral (80+ languages)
- **Best for Long Context (Handling Large Codebases)**: Mistral Large (128K context) or Command A (256K context)
- **Best for Local Code Completion in IDEs**: CodeGemma 2B (optimized for speed)

## Conclusion

Choosing the right local coding model depends on your requirements:
- **If you need the best possible accuracy and have enterprise-level resources**, go with **DeepSeek Coder v2 (236B)**.
- **If you need a powerful open-source coding assistant that is commercially usable**, choose **Qwen 2.5 Coder (32B)**.
- **If you need a balance of speed, accuracy, and licensing freedom**, **Codestral (22B)** and **DeepSeek Coder (33B)** are strong contenders.
- **For lightweight, efficient coding models**, **CodeGemma 7B** or **DeepSeek 6.7B** are great options.
